# Ollama (Local LLM)
# Provides local LLM inference - not an MCP server itself,
# but can be used by eMCP Manager for server detection features
# No API keys required
#
# 1. Add the service block below to docker-compose.yaml under 'services:'
# 2. Run: docker compose up -d ollama

# --- docker-compose service ---
# ollama:
#   image: ollama/ollama:latest
#   container_name: emcp-ollama
#   ports:
#     - "11434:11434"
#   volumes:
#     - ./ollama-data:/root/.ollama
#   environment:
#     - OLLAMA_KEEP_ALIVE=0
#   networks:
#     - emcp-network
#   restart: unless-stopped
#   healthcheck:
#     test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
#     interval: 30s
#     timeout: 10s
#     retries: 3
#
# Also add to emcp-manager environment:
#   - OLLAMA_HOST=http://ollama:11434
